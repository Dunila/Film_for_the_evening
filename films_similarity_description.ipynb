{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90bc75c",
   "metadata": {},
   "source": [
    "# Import, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08f85c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import random \n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as skl_cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4a075ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f510bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "FILMS_DATA_PATH = \"data/films.json\"\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "PRETRAINED_SG_MODEL = True\n",
    "PRETRAINED_LDA_MODEL = True\n",
    "PRETRAINED_BLENDINGNN = True\n",
    "\n",
    "PRETRAINED_SG_PATH = \"models/sgmodel.pth\"\n",
    "PRETRAINED_LDA_PATH = \"models/lda_model.pkl\"\n",
    "PRETRAINED_BLENDINGNN_PATH = \"models/film_prepare_nn_weights_final.pth\"\n",
    "\n",
    "GET_SG_EMB = True\n",
    "GET_BERT_EMB = True\n",
    "GET_LDA_EMB = True\n",
    "GET_BLENDNN_EMB = True\n",
    "\n",
    "SG_EMB_PATH = \"models/sg_embeddings.pt\"\n",
    "BERT_EMB_PATH = \"models/bert_embeddings.pt\"\n",
    "LDA_EMB_PATH = \"models/lda_embeddings.pt\"\n",
    "BLENDNN_EMB_PATH = \"models/films_embeddings.pt\"\n",
    "\n",
    "SAVE_CHANGES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3cb1aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35500, 31)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_films = pd.read_json(FILMS_DATA_PATH)\n",
    "df_films.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "697f2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_watched(value):\n",
    "    if isinstance(value, str):\n",
    "        if \"K\" in value:\n",
    "            return float(value.replace(\"K\", \"\")) * 1e3\n",
    "        elif \"M\" in value:\n",
    "            return float(value.replace(\"M\", \"\")) * 1e6\n",
    "        else:\n",
    "            return float(value)\n",
    "    return value\n",
    "\n",
    "def explode_and_onehot(df, col, prefix):\n",
    "    exploded = df[[\"film_id\", col]].explode(col)\n",
    "    dummies = pd.get_dummies(exploded[col], prefix=prefix)\n",
    "    return exploded[[\"film_id\"]].join(dummies).groupby(\"film_id\").max().astype(int)\n",
    "\n",
    "def films_main_prepare(df_films, log_watched_liked=True):\n",
    "    df_films = df_films.copy()\n",
    "    df_films = df_films.dropna(subset=[\"name\", \"description\"])\n",
    "    df_films[\"name\"] = df_films[\"name\"].apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
    "\n",
    "    df_films[\"watched\"] = df_films[\"watched\"].map(convert_watched)\n",
    "    df_films[\"liked\"] = df_films[\"liked\"].map(convert_watched)\n",
    "\n",
    "    df_films[\"watched\"] = df_films[\"watched\"].fillna(df_films[\"watched\"].mean())\n",
    "    df_films[\"liked\"] = df_films[\"liked\"].fillna(df_films[\"liked\"].mean())\n",
    "    \n",
    "    if log_watched_liked:\n",
    "        df_films[\"log_liked\"] = df_films[\"liked\"].apply(np.log)\n",
    "        df_films[\"log_watched\"] = df_films[\"watched\"].apply(np.log)\n",
    "        df_films = df_films.drop(columns=[\"liked\", \"watched\"])\n",
    "\n",
    "\n",
    "    df_films = df_films.dropna(subset=[\"year\", \"director\"])\n",
    "    df_films[\"year\"] = df_films[\"year\"].astype(np.int32)\n",
    "    df_films[\"decade\"] = df_films[\"year\"] // 10 * 10\n",
    "    df_films[\"decade\"] = df_films[\"decade\"].astype(str)\n",
    "    decade_dummies = pd.get_dummies(df_films[\"decade\"], prefix=\"decade\")\n",
    "    df_films = df_films.drop(columns=\"decade\")\n",
    "    df_films = df_films.join(decade_dummies)\n",
    "\n",
    "\n",
    "    df_films = df_films[(df_films[\"year\"] >= 1920) & (df_films[\"year\"] <= 2024)]\n",
    "    df_films = df_films[(df_films[\"duration\"] >= 60) & (df_films[\"duration\"] <= 240)]\n",
    "\n",
    "    minmax = MinMaxScaler()\n",
    "    df_films[\"year\"] = minmax.fit_transform(df_films[[\"year\"]])\n",
    "\n",
    "    #print(df_films[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']].dtypes)\n",
    "\n",
    "    df_films[\n",
    "        ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    ] = df_films[\n",
    "        ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    ].apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "\n",
    "    df_films[\"cnt_ratings\"] = df_films[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']].sum(axis=1)\n",
    "    df_films = df_films.dropna(subset=[\"cnt_ratings\"])\n",
    "    df_films['1'] = (df_films['1'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['2'] = (df_films['2'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['3'] = (df_films['3'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['4'] = (df_films['4'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['5'] = (df_films['5'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['6'] = (df_films['6'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['7'] = (df_films['7'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['8'] = (df_films['8'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['9'] = (df_films['9'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "    df_films['10'] = (df_films['10'] / df_films[\"cnt_ratings\"]).round(4)\n",
    "\n",
    "    df_films[\"rating\"] = df_films[\"rating\"].fillna(df_films[\"rating\"].mean())\n",
    "\n",
    "    df_films[\"top\"] = df_films[\"top\"].notna().astype(int)\n",
    "\n",
    "    df_films[\"cnt_genres\"] = df_films[\"genres\"].apply(len)\n",
    "    df_films[\"cnt_themes\"] = df_films[\"themes\"].apply(len)\n",
    "    df_films[\"cnt_countries\"] = df_films[\"country\"].apply(len)\n",
    "    df_films[\"cnt_studios\"] = df_films[\"studio\"].apply(len)\n",
    "\n",
    "    df_films[\"film_id\"] = df_films.index\n",
    "\n",
    "    df_films[\"genres\"] = df_films[\"genres\"].apply(lambda x: [g for g in x if g != \"Documentary\"])\n",
    "\n",
    "    genres_ohe = explode_and_onehot(df_films, \"genres\", \"genre\")\n",
    "    countries_ohe = explode_and_onehot(df_films, \"country\", \"country\")\n",
    "    #studios_ohe = explode_and_onehot(df_films, \"studio\", \"studio\")\n",
    "    themes_ohe = explode_and_onehot(df_films, \"themes\", \"theme\")\n",
    "\n",
    "    df_final = df_films.set_index(\"film_id\")\n",
    "    df_final = df_final.join([genres_ohe, countries_ohe, themes_ohe]) \n",
    "\n",
    "    return df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6da8e5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>theme_Tragic sadness and captivating beauty</th>\n",
       "      <th>theme_Twisted dark psychological thriller</th>\n",
       "      <th>theme_Underdog fighting and boxing stories</th>\n",
       "      <th>theme_Underdogs and coming of age</th>\n",
       "      <th>theme_Violent action, guns, and crime</th>\n",
       "      <th>theme_Violent crime and drugs</th>\n",
       "      <th>theme_War and historical adventure</th>\n",
       "      <th>theme_Western frontier dramas with a touch of humor</th>\n",
       "      <th>theme_Westerns</th>\n",
       "      <th>theme_Wild west outlaws and gunfights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barbie</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>Barbie and Ken are having the time of their li...</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parasite</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>All unemployed, Ki-taek’s family takes peculia...</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>The adventures of a group of explorers who mak...</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>A ticking-time-bomb insomniac and a slippery s...</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La La Land</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>Mia, an aspiring actress, serves lattes to mov...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      year                                        description  \\\n",
       "0        Barbie  0.990385  Barbie and Ken are having the time of their li...   \n",
       "1      Parasite  0.951923  All unemployed, Ki-taek’s family takes peculia...   \n",
       "2  Interstellar  0.903846  The adventures of a group of explorers who mak...   \n",
       "3    Fight Club  0.759615  A ticking-time-bomb insomniac and a slippery s...   \n",
       "4    La La Land  0.923077  Mia, an aspiring actress, serves lattes to mov...   \n",
       "\n",
       "        1       2       3       4       5       6       7  ...  \\\n",
       "0  0.0048  0.0113  0.0072  0.0384  0.0369  0.1540  0.1361  ...   \n",
       "1  0.0009  0.0021  0.0009  0.0060  0.0050  0.0352  0.0409  ...   \n",
       "2  0.0015  0.0038  0.0019  0.0125  0.0100  0.0553  0.0550  ...   \n",
       "3  0.0014  0.0037  0.0019  0.0123  0.0105  0.0682  0.0735  ...   \n",
       "4  0.0045  0.0137  0.0047  0.0331  0.0192  0.0988  0.0768  ...   \n",
       "\n",
       "   theme_Tragic sadness and captivating beauty  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "\n",
       "   theme_Twisted dark psychological thriller  \\\n",
       "0                                          0   \n",
       "1                                          1   \n",
       "2                                          0   \n",
       "3                                          1   \n",
       "4                                          0   \n",
       "\n",
       "   theme_Underdog fighting and boxing stories  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "\n",
       "   theme_Underdogs and coming of age  theme_Violent action, guns, and crime  \\\n",
       "0                                  0                                      0   \n",
       "1                                  0                                      0   \n",
       "2                                  0                                      0   \n",
       "3                                  0                                      0   \n",
       "4                                  0                                      0   \n",
       "\n",
       "  theme_Violent crime and drugs theme_War and historical adventure  \\\n",
       "0                             0                                  0   \n",
       "1                             0                                  0   \n",
       "2                             0                                  0   \n",
       "3                             0                                  0   \n",
       "4                             0                                  0   \n",
       "\n",
       "   theme_Western frontier dramas with a touch of humor theme_Westerns  \\\n",
       "0                                                  0                0   \n",
       "1                                                  0                0   \n",
       "2                                                  0                0   \n",
       "3                                                  0                0   \n",
       "4                                                  0                0   \n",
       "\n",
       "  theme_Wild west outlaws and gunfights  \n",
       "0                                     0  \n",
       "1                                     0  \n",
       "2                                     0  \n",
       "3                                     0  \n",
       "4                                     0  \n",
       "\n",
       "[5 rows x 348 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_df = films_main_prepare(df_films)\n",
    "prepared_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "47ce44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sg(text):\n",
    "\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    param \n",
    "        words: Input list of words\n",
    "    return: \n",
    "        Two dictionaries, vocab_to_int, int_to_vocab\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def prob_of_word_to_save(freq, t):\n",
    "    if freq == 0:\n",
    "        return 0\n",
    "    prob = np.sqrt(t/(freq))\n",
    "    rand_element = np.random.rand()\n",
    "    return prob > rand_element\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[.,!?;:()\"]', '', text)\n",
    "    text = re.sub(r\"’s\", \"\", text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1204ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_corpus = prepared_df[\"description\"]\n",
    "description_corpus = description_corpus.apply(lambda x: x.replace(\"\\xa0\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8951fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topk_similar_films(embeddings_matrix, film_idx, criterion=skl_cosine_similarity, k=5):\n",
    "    \"\"\"\n",
    "    From matrix with embeddings of films (with the same indexing as original films dataset)\n",
    "    find k most similar by criterion embeddings and return dict with their indecies and value of similarness\n",
    "    \"\"\"\n",
    "    film_emb = embeddings_matrix[film_idx]\n",
    "    similarities_test = criterion(film_emb.unsqueeze(0), embeddings_matrix)\n",
    "    if k != -1:\n",
    "        topk = similarities_test[0].argsort()[::-1][1:k+1]\n",
    "    else:\n",
    "        topk = similarities_test[0].argsort()[::-1]\n",
    "    return {int(idx): round(float(criterion(embeddings_matrix[film_idx].unsqueeze(0), embeddings_matrix[idx].unsqueeze(0))[0][0]), 5) for idx in topk}\n",
    "\n",
    "def print_similar_films(films_ds, embeddings_matrix, film_idx, criterion=skl_cosine_similarity, k=5):\n",
    "    print(films_ds.iloc[film_idx, 0])\n",
    "    print(\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                f\"{films_ds.iloc[idx, 0]} -  {value}\" \n",
    "                for idx, value in find_topk_similar_films(embeddings_matrix, film_idx, criterion=criterion, k=k).items()\n",
    "            ] #sorted()?\n",
    "        )\n",
    "    )\n",
    "\n",
    "def pair_film_similarity(film_idx1, film_idx2, films_matrix, embedding_matrix, criterion=skl_cosine_similarity, print_=True):\n",
    "    film_emb1 = embedding_matrix[film_idx1].unsqueeze(0)\n",
    "    film_emb2 = embedding_matrix[film_idx2].unsqueeze(0)\n",
    "\n",
    "    similarity = criterion(film_emb1, film_emb2)[0][0]\n",
    "    if print_:\n",
    "        print(\n",
    "            f\"{films_matrix.iloc[film_idx1, 0]} - {films_matrix.iloc[film_idx2, 0]} - {similarity:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93adece",
   "metadata": {},
   "source": [
    "## Skiprgamms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20dc6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_film_embedding_sg(description, model, vocab_to_int):\n",
    "    indicies = [vocab_to_int.get(word, vocab_to_int[\"<PERIOD>\"]) for word in description.split(\" \")]\n",
    "    tensor = torch.LongTensor(indicies)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embed(tensor)\n",
    "        desc_embedding = embeddings.mean(dim=0)\n",
    "    return desc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "32e921cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    \"\"\"\n",
    "    Get a list of words in a window of index\n",
    "    \"\"\"\n",
    "\n",
    "    right_side = words[idx+1:min(len(words),idx+window_size+1)]\n",
    "    left_side = words[max(0, idx-window_size):idx]\n",
    "    return left_side + right_side\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    \"\"\"Create a generator of batches as a tuple (inputs, targets)\"\"\"\n",
    "    n_batches = len(words) // batch_size\n",
    "\n",
    "    words = words[: n_batches * batch_size]\n",
    "\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx : idx + batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "        yield x, y\n",
    "\n",
    "def my_cosine_similarity(embedding, valid_size=16, valid_window=100):\n",
    "    embed_vectors = embedding.weight\n",
    "    vocab_size = embed_vectors.shape[0]\n",
    "\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "\n",
    "    max_index = min(vocab_size, 1000 + valid_window)\n",
    "    valid_pool = list(range(min(valid_window, vocab_size))) + list(range(1000, max_index))\n",
    "\n",
    "    valid_examples = np.random.choice(valid_pool, size=valid_size, replace=False)\n",
    "    valid_examples = torch.LongTensor(valid_examples)\n",
    "\n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t()) / magnitudes\n",
    "\n",
    "    return valid_examples, similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f19dfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySkipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(self.embed(x))\n",
    "\n",
    "def train_skip_grams(model, train_words, int_to_vocab, criterion, optimizer, n_epochs=15, batch_size=1024, print_every=300):\n",
    "    mean_loss = []\n",
    "    steps = 0\n",
    "    for epoch in trange(n_epochs, leave=True, desc=\"Epoch number: \"):\n",
    "        pbar = tqdm(\n",
    "            get_batches(train_words, batch_size),\n",
    "            leave=False,\n",
    "            desc=\"Batch number\",\n",
    "            total=len(train_words) // batch_size\n",
    "        )\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        for inputs, targets in pbar:\n",
    "            steps += 1\n",
    "            inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "\n",
    "            log_ps = model(inputs)\n",
    "            loss = criterion(log_ps, targets)\n",
    "            epoch_loss.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                valid_examples, valid_similarities = my_cosine_similarity(model.embed)\n",
    "                _, closest_idxs = valid_similarities.topk(5)\n",
    "\n",
    "                lines_to_write = []\n",
    "                for ii, valid_idx in enumerate(valid_examples):\n",
    "                    closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                    lines_to_write.append(int_to_vocab[valid_idx.item()] + \" | \" + \", \".join(closest_words))\n",
    "                \n",
    "                # clear_output(wait=True)\n",
    "\n",
    "                for line in lines_to_write:\n",
    "                    pbar.write(line)\n",
    "                pbar.write(\"...\")\n",
    "\n",
    "def get_film_embedding_sg(description, model, vocab_to_int):\n",
    "    indicies = [vocab_to_int.get(word, vocab_to_int[\"<PERIOD>\"]) for word in description.split(\" \")]\n",
    "    tensor = torch.LongTensor(indicies)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embed(tensor)\n",
    "        desc_embedding = embeddings.mean(dim=0)\n",
    "    return desc_embedding\n",
    "\n",
    "def get_avg_skip_gramms(descriptions, embedding_dim, n_epochs=5, pretrained_model=None):\n",
    "    words = preprocess_sg(\" \".join(descriptions))\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "    int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "    threshold = 1e-5\n",
    "    word_counts = Counter(int_words)\n",
    "\n",
    "    train_words = [word for word in int_words if prob_of_word_to_save(word_counts[word]/len(int_words), threshold)]\n",
    "\n",
    "    if PRETRAINED_SG_MODEL:\n",
    "        model = pretrained_model\n",
    "    else:\n",
    "        model = MySkipgram(len(vocab_to_int), embedding_dim)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "        train_skip_grams(model, train_words, int_to_vocab, criterion, optimizer, n_epochs=n_epochs)\n",
    "    \n",
    "    skip_gramms_embeddings_matrix = torch.stack([\n",
    "        get_film_embedding_sg(desc, model, vocab_to_int) for desc in description_corpus\n",
    "    ])\n",
    "    return model, skip_gramms_embeddings_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8e0158da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED_SG_MODEL:\n",
    "    sg_model = torch.load(PRETRAINED_SG_PATH, weights_only=False)\n",
    "\n",
    "if GET_SG_EMB:\n",
    "    sgramms = torch.load(SG_EMB_PATH)\n",
    "\n",
    "if PRETRAINED_SG_MODEL and not GET_SG_EMB:\n",
    "    sg_model, sgramms = get_avg_skip_gramms(description_corpus, 128, 25, pretrained_model=sg_model)\n",
    "\n",
    "if not PRETRAINED_SG_MODEL and not GET_SG_EMB:\n",
    "    sg_model, sgramms = get_avg_skip_gramms(description_corpus, 128, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "50e10f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3191, -0.1515,  0.0500,  ...,  0.0027,  0.1108,  0.0089],\n",
       "        [-0.1453, -0.1262,  0.0968,  ...,  0.1479, -0.0615, -0.0273],\n",
       "        [ 0.0602,  0.0041,  0.0097,  ..., -0.2327,  0.0156, -0.0532],\n",
       "        ...,\n",
       "        [-0.2435, -0.1436,  0.0558,  ..., -0.0124,  0.0718, -0.1726],\n",
       "        [-0.0501, -0.0521, -0.3055,  ...,  0.0663,  0.2914, -0.0880],\n",
       "        [-0.1449, -0.0901,  0.0508,  ...,  0.1371, -0.0475, -0.1074]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "91dd250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{21839: 0.69166, 11539: 0.6887, 3290: 0.68494, 16210: 0.68291, 7087: 0.68041}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_topk_similar_films(sgramms, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3bdb7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fight Club\n",
      "The Connection -  0.70258\n",
      "The Orphanage -  0.67978\n",
      "A Hard Day’s Night -  0.67954\n",
      "Next Door -  0.67658\n",
      "The Sound of Music -  0.67407\n"
     ]
    }
   ],
   "source": [
    "print_similar_films(prepared_df, sgramms, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ace30655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring - Avengers: Age of Ultron - 0.6555\n"
     ]
    }
   ],
   "source": [
    "pair_film_similarity(71, 236, prepared_df, sgramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2414eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_films[(df_films[\"name\"].notna()) & (df_films[\"name\"].str.contains(\"Casino\"))].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827b142",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dde27c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.squeeze(0)\n",
    "\n",
    "def get_bert_embeddings(descriptions, embedding_dim=128):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    films_embeddings_matrix = torch.stack([\n",
    "        get_bert_embedding(desc, tokenizer, bert) for desc in tqdm(descriptions)\n",
    "    ])\n",
    "    return bert, films_embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "69533a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GET_BERT_EMB:\n",
    "    bert_emb = torch.load(BERT_EMB_PATH)\n",
    "else:\n",
    "    bert, bert_emb = get_bert_embeddings(description_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7993c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert, bert_emb = get_bert_embeddings(description_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "be0eaade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{29034: 0.91709, 212: 0.91533, 19342: 0.91372, 2660: 0.9134, 30063: 0.91274}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_topk_similar_films(bert_emb, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "50c7c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring\n",
      "The Hobbit: An Unexpected Journey -  0.92037\n",
      "Jason and the Argonauts -  0.91415\n",
      "Happily Ever After -  0.91365\n",
      "The Hobbit: The Battle of the Five Armies -  0.90962\n",
      "Princess Mononoke -  0.90904\n"
     ]
    }
   ],
   "source": [
    "print_similar_films(prepared_df, bert_emb, 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f76a206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring - Fargo - 0.7138\n"
     ]
    }
   ],
   "source": [
    "pair_film_similarity(71, 235, prepared_df, bert_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54edc31",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e4e073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10, k_topics=5):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    counter = 0\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"_____________________\")\n",
    "        print(f\"Theme: {idx+1}\")\n",
    "        top_words = topic.argsort()[:-top_n - 1:-1]\n",
    "        for i in top_words:\n",
    "            print(f\"{words[i]} ({topic[i]:.2f})\")\n",
    "        counter += 1\n",
    "        if counter == k_topics:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e7757752",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = description_corpus.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d93e07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10, k_topics=5):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    counter = 0\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"_____________________\")\n",
    "        print(f\"Theme: {idx+1}\")\n",
    "        top_words = topic.argsort()[:-top_n - 1:-1]\n",
    "        for i in top_words:\n",
    "            print(f\"{words[i]} ({topic[i]:.2f})\")\n",
    "        counter += 1\n",
    "        if counter == k_topics:\n",
    "            break\n",
    "\n",
    "def get_top_k_topics(topic_distribution, k=3):\n",
    "    top_indices = np.argsort(topic_distribution)[::-1][:k]\n",
    "    top_probs = topic_distribution[top_indices]\n",
    "    return list(zip(top_indices, top_probs))\n",
    "\n",
    "def print_topic_by_id(model, vectorizer, idx, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic = model.components_[idx]\n",
    "    print(f\"Theme: {idx}\")\n",
    "    top_words = topic.argsort()[:-top_n - 1:-1]\n",
    "    for i in top_words:\n",
    "        print(f\"{words[i]} ({topic[i]:.2f})\")\n",
    "    print()\n",
    "\n",
    "def get_lda_embedding(desc, model, vectorizer):\n",
    "    clean_desc = preprocess(desc)\n",
    "    new_vec = vectorizer.transform([clean_desc])\n",
    "    emb = model.transform(new_vec)[0]\n",
    "    return emb\n",
    "\n",
    "def get_lda_matrix(clean_corpus, fitted_vectorizer, pretrained_model=None):\n",
    "    x = fitted_vectorizer.transform(clean_corpus)\n",
    "\n",
    "    if PRETRAINED_LDA_MODEL:\n",
    "        lda = pretrained_model\n",
    "    else:\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=256,\n",
    "            random_state=42,\n",
    "            learning_method='batch',\n",
    "            max_iter=20\n",
    "        )\n",
    "        lda.fit(x)\n",
    "    films_embeddings_matrix_lda = torch.stack([\n",
    "        torch.tensor(get_lda_embedding(desc, lda, fitted_vectorizer), dtype=torch.float32) for desc in tqdm(description_corpus)\n",
    "    ])\n",
    "    return lda, films_embeddings_matrix_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "86299ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_df=0.9, min_df=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(max_df=0.9, min_df=5)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_df=0.9, min_df=5)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.9, min_df=5)\n",
    "vectorizer.fit(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a69b6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED_LDA_MODEL:\n",
    "    lda = joblib.load(PRETRAINED_LDA_PATH)\n",
    "\n",
    "if GET_LDA_EMB:\n",
    "    lda_matrix = torch.load(LDA_EMB_PATH)\n",
    "\n",
    "if PRETRAINED_LDA_MODEL and not GET_LDA_EMB:\n",
    "    lda, lda_matrix = get_lda_matrix(clean_corpus, vectorizer, pretrained_model=lda)\n",
    "\n",
    "if not PRETRAINED_LDA_MODEL and not GET_LDA_EMB:\n",
    "    lda, lda_matrix = get_lda_matrix(clean_corpus, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7c953a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda, lda_matrix = get_lda_matrix(clean_corpus, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1f26cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Return of the King - The Lord of the Rings: The Two Towers - 0.2015\n"
     ]
    }
   ],
   "source": [
    "pair_film_similarity(136, 150, prepared_df, lda_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d3a557b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 3.9081e-01,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 3.4189e-02,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.9651e-01, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 7.5495e-02, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        9.2834e-02, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 7.8371e-02, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 9.9373e-02, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04,\n",
       "        1.3021e-04, 1.3021e-04, 1.3021e-04, 1.3021e-04])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_matrix[71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f5e691c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(11), np.float64(0.3908102939073579)),\n",
       " (np.int64(105), np.float64(0.19650576111446902)),\n",
       " (np.int64(224), np.float64(0.09937300143840277))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_topics(\n",
    "    lda.transform(\n",
    "        vectorizer.transform(\n",
    "            [clean_corpus[71]]\n",
    "            )\n",
    "        )[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d0a85cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: 11\n",
      "king (484.60)\n",
      "evil (328.48)\n",
      "\n",
      "Theme: 105\n",
      "terrorist (127.92)\n",
      "pursuit (85.21)\n",
      "\n",
      "Theme: 224\n",
      "charles (35.56)\n",
      "young (29.76)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in [11, 105, 224]:\n",
    "    print_topic_by_id(lda, vectorizer, topic, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0ab9e47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________\n",
      "Theme: 1\n",
      "convicted (74.48)\n",
      "framed (69.03)\n",
      "property (53.48)\n",
      "reclusive (36.76)\n",
      "family (36.53)\n",
      "_____________________\n",
      "Theme: 2\n",
      "park (204.00)\n",
      "weekend (182.00)\n",
      "reunite (133.91)\n",
      "mountain (129.34)\n",
      "group (81.90)\n",
      "_____________________\n",
      "Theme: 3\n",
      "los (335.00)\n",
      "angeles (327.00)\n",
      "rises (73.63)\n",
      "crazed (27.34)\n",
      "streets (23.09)\n",
      "_____________________\n",
      "Theme: 4\n",
      "believe (131.47)\n",
      "ten (82.07)\n",
      "string (75.62)\n",
      "years (56.89)\n",
      "gone (49.88)\n",
      "_____________________\n",
      "Theme: 5\n",
      "town (230.72)\n",
      "small (144.02)\n",
      "teens (122.41)\n",
      "sheriff (105.27)\n",
      "aliens (105.00)\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda, vectorizer, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f32e155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{19591: 0.83123, 5650: 0.8228, 25988: 0.82112, 28836: 0.82011, 8965: 0.81774}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_topk_similar_films(lda_matrix, 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5f261d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring\n",
      "Rampant -  0.83123\n",
      "Shadow -  0.8228\n",
      "Tower of London -  0.82112\n",
      "A Ravaging Wind -  0.82011\n",
      "Big Fish & Begonia -  0.81774\n"
     ]
    }
   ],
   "source": [
    "print_similar_films(prepared_df, lda_matrix, 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2d0de89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring - The Lord of the Rings: The Two Towers - 0.2209\n"
     ]
    }
   ],
   "source": [
    "pair_film_similarity(71, 150, prepared_df, lda_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830321a",
   "metadata": {},
   "source": [
    "## Blending NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "882b9b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30910, 128]), torch.Size([30910, 768]), torch.Size([30910, 256]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgramms.size(), bert_emb.size(), lda_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0e5505d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30910, 348)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7d34fd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30910, 1152])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films_embeddings = torch.cat(\n",
    "    [sgramms, bert_emb, lda_matrix], dim=1\n",
    ")\n",
    "films_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "94fb7166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30910,), (30910,), (30910, 18), (30910, 109))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_year = prepared_df[\"year\"]\n",
    "y_rating = prepared_df[\"rating\"]\n",
    "genre_columns = prepared_df.columns[prepared_df.columns.str.startswith(\"genre_\")]\n",
    "themes_columns = prepared_df.columns[prepared_df.columns.str.startswith(\"theme_\")]\n",
    "y_genres = prepared_df[genre_columns]\n",
    "y_themes = prepared_df[themes_columns]\n",
    "y_year.shape, y_rating.shape, y_genres.shape, y_themes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cc3eba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFilm(torch.utils.data.Dataset):\n",
    "    def __init__(self, films_embs, genres, themes, year, rating):\n",
    "        self.X = films_embs\n",
    "        self.y_genres = torch.tensor(genres.values) if isinstance(genres, pd.DataFrame) else genres\n",
    "        self.y_themes = torch.tensor(themes.values) if isinstance(themes, pd.DataFrame) else themes\n",
    "        self.y_year = torch.tensor(year.values) if isinstance(year, pd.Series) else year\n",
    "        self.y_rating = torch.tensor(rating.values) if isinstance(rating, pd.Series) else rating\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_year)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.X[index],\n",
    "            self.y_genres[index].float(),\n",
    "            self.y_themes[index].float(),\n",
    "            self.y_year[index].float(),\n",
    "            self.y_rating[index].float(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "47c917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetFilm(films_embeddings, y_genres, y_themes, y_year, y_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cfa1bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4de0ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskFilm(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_genres, num_themes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prepare_emb = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # nn.Linear(1024, 1024),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(1024,  hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.genre_head = nn.Linear(hidden_dim, num_genres)\n",
    "        self.theme_head = nn.Linear(hidden_dim, num_themes)\n",
    "        self.year_head = nn.Linear(hidden_dim, 1)\n",
    "        self.rating_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prepare_emb(x)\n",
    "        return (\n",
    "            self.genre_head(x),\n",
    "            self.theme_head(x),\n",
    "            self.year_head(x),\n",
    "            self.rating_head(x)\n",
    "        )\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.prepare_emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7729a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNT_GENRES = 18\n",
    "CNT_THEMES = 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "39019cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = MultiTaskFilm(1152, 512, CNT_GENRES, CNT_THEMES)\n",
    "optimizer = torch.optim.SGD(blender.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c9b4dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title=\"loss\"):\n",
    "    plt.figure()\n",
    "    plt.title(\"{}\".format(title))\n",
    "    plt.plot(train_history, label=\"train\", zorder=1)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def train(model, optimizer, train_dataloader, test_dataloader, n_epochs=5):\n",
    "    loss_genere_log, loss_theme_log, loss_year_log, loss_rating_log = [], [], [], []\n",
    "    mean_loss_log = []\n",
    "    val_loss_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training, epoch {epoch}\", leave=False):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            film, y_genres, y_themes, y_year, y_rating = batch\n",
    "            pred_genres, pred_themes, pred_year, pred_rating = model(film)\n",
    "\n",
    "            loss_genre = nn.BCEWithLogitsLoss()(pred_genres, y_genres.float())\n",
    "            loss_theme = nn.BCEWithLogitsLoss()(pred_themes, y_themes.float())\n",
    "            loss_year = nn.MSELoss()(pred_year.squeeze(), y_year)\n",
    "            loss_rating = nn.MSELoss()(pred_rating.squeeze(), y_rating)\n",
    "\n",
    "            loss = (loss_genre + loss_theme + loss_year + loss_rating)/4\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_genere_log.append(loss_genre)\n",
    "            loss_theme_log.append(loss_theme)\n",
    "            loss_year_log.append(loss_year)\n",
    "            loss_rating_log.append(loss_rating)\n",
    "            #mean_loss_log.append(loss)\n",
    "            train_epoch_loss.append(loss.item())\n",
    "\n",
    "            # pbar.set_postfix({\n",
    "            #     \"genre\": f\"{loss_genre.item():.4f}\",\n",
    "            #     \"theme\": f\"{loss_theme.item():.4f}\",\n",
    "            #     \"year\": f\"{loss_year.item():.4f}\",\n",
    "            #     \"rating\": f\"{loss_rating.item():.4f}\",\n",
    "            #     \"mean\": f\"{loss.item():.4f}\",\n",
    "            # })\n",
    "\n",
    "        val_loss_genere_log, val_loss_theme_log, val_loss_year_log, val_loss_rating_log = [], [], [], []\n",
    "        val_loss_log = []\n",
    "        val_epoch_loss = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_dataloader, desc=f\"Training, epoch {epoch}\", leave=False):\n",
    "            \n",
    "                film, y_genres, y_themes, y_year, y_rating = batch\n",
    "                pred_genres, pred_themes, pred_year, pred_rating = model(film)\n",
    "\n",
    "                loss_genre = nn.BCEWithLogitsLoss()(pred_genres, y_genres.float())\n",
    "                loss_theme = nn.BCEWithLogitsLoss()(pred_themes, y_themes.float())\n",
    "                loss_year = nn.MSELoss()(pred_year.squeeze(), y_year)\n",
    "                loss_rating = nn.MSELoss()(pred_rating.squeeze(), y_rating)\n",
    "\n",
    "                loss = (loss_genre + loss_theme + loss_year + loss_rating)/4\n",
    "\n",
    "                val_loss_genere_log.append(loss_genre)\n",
    "                val_loss_theme_log.append(loss_theme)\n",
    "                val_loss_year_log.append(loss_year)\n",
    "                val_loss_rating_log.append(loss_rating)\n",
    "                val_loss_log.append(loss)\n",
    "            val_epoch_loss.append(sum(val_loss_log)/len(val_loss_log))\n",
    "\n",
    "\n",
    "            train_mean = np.mean(train_epoch_loss)\n",
    "            mean_loss_log.append(train_mean)\n",
    "            val_mean = np.mean(val_loss_log)\n",
    "            val_loss_log.append(val_mean)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            plot_history(mean_loss_log, val_loss_log, \"loss\")\n",
    "\n",
    "        # clear_output()\n",
    "\n",
    "        # print(f\"Train loss (epoch {epoch}):\", train_mean)\n",
    "        # print(f\"Val loss (epoch {epoch}):\", val_mean)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(f\"Train loss (epoch {epoch}):\")\n",
    "        print(f\"  genre:  {np.mean([loss.detach().item() for loss in loss_genere_log[-len(train_epoch_loss):]]):.4f}\")\n",
    "        print(f\"  theme:  {np.mean([loss.detach().item() for loss in loss_theme_log[-len(train_epoch_loss):]]):.4f}\")\n",
    "        print(f\"  year:   {np.mean([loss.detach().item() for loss in loss_year_log[-len(train_epoch_loss):]]):.4f}\")\n",
    "        print(f\"  rating: {np.mean([loss.detach().item() for loss in loss_rating_log[-len(train_epoch_loss):]]):.4f}\")\n",
    "        print(f\"  mean:   {train_mean:.4f}\")\n",
    "\n",
    "        print(f\"\\nVal loss (epoch {epoch}):\")\n",
    "        print(f\"  genre:  {np.mean([loss.detach().item() for loss in val_loss_genere_log]):.4f}\")\n",
    "        print(f\"  theme:  {np.mean([loss.detach().item() for loss in val_loss_theme_log]):.4f}\")\n",
    "        print(f\"  year:   {np.mean([loss.detach().item() for loss in val_loss_year_log]):.4f}\")\n",
    "        print(f\"  rating: {np.mean([loss.detach().item() for loss in val_loss_rating_log]):.4f}\")\n",
    "        print(f\"  mean:   {val_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "faabde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED_BLENDINGNN:\n",
    "    blender.load_state_dict(torch.load(PRETRAINED_BLENDINGNN_PATH))\n",
    "else:\n",
    "    train(blender, optimizer, train_dataloader, test_dataloader, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e0a2edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GET_BLENDNN_EMB:\n",
    "    nn_embeddings = torch.load(BLENDNN_EMB_PATH)\n",
    "else:\n",
    "    nn_embeddings = torch.stack([\n",
    "        blender.get_embedding(emb).detach() for emb in films_embeddings\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1b7b2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_CHANGES:\n",
    "    torch.save(blender.state_dict(), \"film_prepare_nn_weights_final.pth\")\n",
    "\n",
    "    torch.save(films_embeddings, \"films_embeddings.pt\")\n",
    "    torch.save(lda_matrix, \"lda_embeddings.pt\")\n",
    "    torch.save(bert_emb, \"bert_embeddings.pt\")\n",
    "    torch.save(sgramms, \"sg_embeddings.pt\")\n",
    "\n",
    "    torch.save(sg_model, \"sgmodel.pth\")\n",
    "    joblib.dump(lda, \"models/lda_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ceccf",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7d7b2a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2008,  0.6989,  0.2834,  0.3809,  0.4038,  0.0073,  0.3233,  0.1286,\n",
       "         0.2826,  0.3149,  0.4274,  0.7273,  0.2387, -0.1917,  0.1168,  0.2919,\n",
       "        -0.1565, -0.3877,  0.6940, -0.4775,  0.0930, -0.1887, -0.4946,  0.5394,\n",
       "         0.1135, -0.1649,  0.1573, -0.0738, -0.7115, -0.1881, -0.6489, -0.2542,\n",
       "         0.5220, -0.4802,  0.4658, -0.1466,  0.1152, -0.0405,  0.1043,  0.2168,\n",
       "         0.5043, -0.5603,  0.3438,  0.9217,  0.5905, -0.2420,  0.3028, -0.0027,\n",
       "        -0.4311,  0.0638, -0.0950,  0.3762,  0.9601, -0.4661,  0.4256,  0.3660,\n",
       "         0.2240, -0.0157,  0.4371,  0.1747,  0.5872, -0.1158,  0.4128,  0.2197,\n",
       "         0.0770,  0.3567, -0.1960,  0.7006,  0.2464, -0.0313, -0.4827, -0.5695,\n",
       "        -0.0864,  0.1100,  0.1809, -0.4178, -0.8826, -0.1242, -0.2817, -0.1705,\n",
       "         0.7734, -0.0419,  0.1353, -0.1761, -0.0544, -0.1061,  0.0548, -0.3134,\n",
       "        -0.3163, -0.5995,  0.2614, -0.2326, -0.7287, -0.2513, -0.6858,  0.4414,\n",
       "         0.5033,  0.2642, -0.9918, -0.1897,  0.0860,  0.2879,  0.2725,  0.3388,\n",
       "         0.3366,  0.5266, -0.3837,  0.5708, -0.3018, -0.2211, -0.1560,  0.0572,\n",
       "        -0.2874,  0.3907,  0.0860, -0.9159, -0.3415,  0.6253, -0.5081,  0.1024,\n",
       "         0.8499, -0.7542, -0.7324, -0.0214,  0.0914,  0.3679,  0.1748, -0.0068,\n",
       "         0.0215,  0.0066,  0.6419,  0.4148,  0.3018,  0.2005,  0.4564,  0.0692,\n",
       "         0.1928, -0.6529,  0.4595,  0.6263, -0.0255, -0.4019, -0.1690,  0.3567,\n",
       "         0.4731, -0.1852, -0.2659, -0.3581,  0.4926,  0.2811, -0.5584,  0.5473,\n",
       "         0.0604, -0.0858, -0.4770,  0.1040, -0.6531, -0.0221,  0.1617, -0.0034,\n",
       "        -0.1643,  0.4186,  0.4767, -0.2953,  0.2011, -0.4480,  0.2830,  0.7247,\n",
       "        -0.3402,  0.2040,  0.0191,  0.4461,  0.5547, -0.3039, -0.5931, -0.0704,\n",
       "         0.0570, -0.5169, -0.3368,  0.3792,  0.0216,  0.3453,  0.5071, -0.6215,\n",
       "        -0.5283,  0.2535, -0.1265,  0.3425,  0.0133, -0.2449,  0.0221, -0.0022,\n",
       "        -0.3105, -0.4972, -0.3496, -0.3451,  0.1891,  0.1562,  0.3888,  0.4774,\n",
       "        -0.3447, -0.1185, -0.4129,  0.2154,  0.5505,  0.6897,  0.1837,  0.2569,\n",
       "         0.2560,  0.1849,  0.2351,  0.3940, -0.1221, -0.2981, -0.2272, -0.0531,\n",
       "         0.3188,  0.6045,  0.2546,  0.1281, -0.2108,  0.6890, -0.9091, -0.3682,\n",
       "        -0.7829, -0.1474, -0.0657, -0.1434,  0.4745,  0.3501, -0.4182, -0.5014,\n",
       "        -0.2687,  0.2654, -0.4933,  0.5917, -0.3345,  0.2254, -0.4997,  0.0021,\n",
       "        -0.3156,  0.0490, -0.0821, -0.6127,  0.4859,  0.1171, -0.4670,  0.7937,\n",
       "        -0.2081,  0.5531, -0.2965,  0.1686, -0.3678, -0.2765, -0.6837, -0.5711,\n",
       "         0.5863, -0.0459, -0.2206,  0.2131,  0.0093,  0.1703, -0.5600, -0.3530,\n",
       "         0.0382, -0.4822, -0.3374,  0.3801, -0.2105,  0.3428,  0.7639,  0.1596,\n",
       "         0.1861,  0.0874, -0.0248, -0.6093,  0.6823, -0.1632, -0.2092, -0.1135,\n",
       "         0.0698, -0.1692,  0.2385,  0.3951,  0.0266,  0.1529,  0.1888, -0.8269,\n",
       "         0.1970, -0.1224,  0.1034,  0.6137, -0.5356,  0.1814, -0.4824, -0.1767,\n",
       "        -0.3094,  0.2964,  0.2177,  0.2623,  0.0416,  0.4263,  0.2660, -0.4605,\n",
       "         0.1957, -0.1065, -0.2802,  0.2050, -0.4965,  0.4410,  0.0883,  0.1301,\n",
       "         0.8309,  0.4352,  0.2409,  0.2850,  0.0024,  0.0065, -0.0157, -0.0537,\n",
       "        -0.3212, -0.2683,  0.4132,  0.0686, -0.4237,  0.4802,  0.0015, -1.0197,\n",
       "         0.6617, -0.0326, -0.6565, -0.4136,  0.2561, -0.3968, -0.4533, -0.1814,\n",
       "        -0.0163,  0.0708, -0.1380, -0.0671, -0.0854,  0.4550,  0.6243, -0.6289,\n",
       "        -0.3414,  0.5740,  0.1826, -0.4536,  0.1198,  0.1506,  0.3339, -0.2371,\n",
       "        -0.8782, -0.1766,  0.1760, -0.2471, -0.7166,  0.5310, -0.4374, -0.2153,\n",
       "        -0.8879, -0.2977, -0.7479, -0.3504,  0.3542, -0.1444,  0.4386, -0.1148,\n",
       "         0.1348,  0.1093, -0.0112,  0.6853,  0.3275,  0.5463, -0.3678,  0.3357,\n",
       "        -0.3880,  0.6628, -0.1353,  0.3030, -0.2315,  0.0504,  0.5128, -0.1383,\n",
       "         0.0760, -0.5011,  0.0847, -0.0368,  0.6270,  0.3335, -0.3499, -0.2360,\n",
       "        -0.2450,  0.2377, -0.1352, -0.2800,  0.5414, -0.2118, -0.1795, -0.1690,\n",
       "         0.0274,  0.0768,  0.6132,  0.1007,  0.0184, -0.6517,  0.3675, -0.3869,\n",
       "         0.2102,  0.3300, -0.2525,  0.1534,  0.0784, -0.2351, -0.7255, -0.2850,\n",
       "         0.4353, -0.4497, -0.4023, -0.8418,  0.1605, -0.2740, -0.6993, -0.7545,\n",
       "         0.1816, -0.1053,  0.0505,  0.3893, -0.2533, -0.8054, -0.4542, -0.4543,\n",
       "        -0.3571, -0.1220, -0.2838, -0.4359,  0.8255,  0.3495,  0.1174, -0.1714,\n",
       "         0.6114, -0.2228,  0.1897, -0.7492,  0.4179,  0.0766,  0.2052,  0.0035,\n",
       "         0.3149, -0.6030, -0.0971, -0.4131, -0.5069, -0.5801, -0.1111, -0.8276,\n",
       "         0.3920, -0.0173, -0.2630,  0.1074, -0.0762, -0.4082,  0.4553,  0.0615,\n",
       "         0.1256, -0.2128, -0.0358,  0.0492,  0.2823,  0.3652, -0.0810, -0.5654,\n",
       "        -0.2036, -0.5618,  0.2616, -0.6533,  0.5262, -0.4049,  0.0048, -0.4149,\n",
       "        -0.5910, -0.2279,  0.7331,  0.2043, -0.0793,  0.3710,  0.1464,  0.1030,\n",
       "        -0.6226,  0.1536,  0.5093,  0.5141,  0.2661,  0.0503,  0.4552,  0.4198,\n",
       "         0.0471, -0.6176,  0.7502,  0.4079, -0.2309, -0.0912,  0.3299, -0.3521,\n",
       "        -0.3614,  0.1495, -0.3864, -0.4182, -0.3807,  0.3903,  0.2249, -0.2951])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 71\n",
    "test_emb = films_embeddings[test_idx]\n",
    "blender.get_embedding(test_emb).data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "893759e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{539: 0.91705, 29859: 0.91194, 5404: 0.91159, 183: 0.90762, 711: 0.907}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_topk_similar_films(nn_embeddings, 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "75f6bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring\n",
      "The Hobbit: An Unexpected Journey -  0.91705\n",
      "Happily Ever After -  0.91194\n",
      "Jason and the Argonauts -  0.91159\n",
      "Princess Mononoke -  0.90762\n",
      "The Hobbit: The Battle of the Five Armies -  0.907\n",
      "Pan -  0.90643\n",
      "The Chronicles of Narnia: The Lion, the Witch and the Wardrobe -  0.90554\n",
      "Tinker Bell and the Legend of the NeverBeast -  0.9049\n",
      "Ninja -  0.90457\n",
      "Beyond Skyline -  0.90411\n"
     ]
    }
   ],
   "source": [
    "print_similar_films(prepared_df, nn_embeddings, 71, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9290c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lord of the Rings: The Fellowship of the Ring - The Lord of the Rings: The Two Towers - 0.7014\n"
     ]
    }
   ],
   "source": [
    "pair_film_similarity(71, 150, prepared_df, nn_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "79f42e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spider-Man: Across the Spider-Verse\n",
      "Superman II -  0.88189\n",
      "Dark Phoenix -  0.87512\n",
      "Spider-Man 3 -  0.87484\n",
      "Hellboy II: The Golden Army -  0.87121\n",
      "Superman: Brainiac Attacks -  0.87116\n",
      "The Dark Knight -  0.87114\n",
      "Legion of Super-Heroes -  0.86859\n",
      "The Dark Knight Rises -  0.8655\n",
      "Logan -  0.86448\n",
      "Iron Man: Rise of Technovore -  0.86417\n"
     ]
    }
   ],
   "source": [
    "print_similar_films(prepared_df, nn_embeddings, 24, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "13e0cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spider-Man: Across the Spider-Verse\n",
      "Superman II -  0.88347\n",
      "Spider-Man 3 -  0.87807\n",
      "Dark Phoenix -  0.87725\n",
      "Superman: Brainiac Attacks -  0.87447\n",
      "The Dark Knight -  0.87415\n",
      "Hellboy II: The Golden Army -  0.87366\n",
      "Legion of Super-Heroes -  0.87037\n",
      "The Dark Knight Rises -  0.8677\n",
      "Iron Man: Rise of Technovore -  0.8673\n",
      "Logan -  0.86717\n"
     ]
    }
   ],
   "source": [
    "print_similar_films(prepared_df, bert_emb, 24, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99074d6",
   "metadata": {},
   "source": [
    "Result of blending NN is similar to BERT embeddings. Despite BERT is already powerfull pretrained model, I'm not fully satisfied with results. Maybe it is becouse of small amount of data and we could parse TFDB or IMDB with full synopsises, which are double size larger than regular descriptions from LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7e30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
